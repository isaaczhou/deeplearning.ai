{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Binary Classification\n",
    "- Unroll pixel matrices (red, green, blue each is a 64 x 64 matrix) into a 64 x 64 x 3 vector $n_x = 12288$\n",
    "- Notation\n",
    "    - (x,y), where $ x \\in \\mathbb{R}^{n_x} $ and $ y \\in \\{0,1\\} $\n",
    "    - m training sets: $ \\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}),...,(x^{(m)}, y^{(m)})\\} $\n",
    "    - $m_{train}$ and $ m_{test} $\n",
    "    - $X \\in \\mathbb{R}^{n_x \\times m}$ and $Y \\in \\mathbb{R}^{1 \\times m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Logistic Regression\n",
    "- Giving x, predict $\\hat{y} = P (y=1|x)$, based on x what is the chance that y is...\n",
    "- $x \\in \\mathbb(R)^{n_x}$\n",
    "- Parameters: $w \\in \\mathbb{R}^{n_x}, b \\in \\mathbb{R}$\n",
    "- Not Linear Functions: If $\\hat{y} = w^Tx + b$ is used for linear regression, but here $\\hat{y} \\notin {(0,1)}$ \n",
    "- Sigmoid Function: $f(x) = \\frac{1}{1+e^{-x}} $\n",
    "    - if x is a very large positive number f(x) approach 1\n",
    "    - if x is a very large negative number f(x) will approach 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Logistic Regression Cost Function\n",
    "- $\\hat{y} = f(w^Tx + b)$ where $f(x) = \\frac{1}{1+e^{-x}}$\n",
    "- Given m training sets: $ \\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}),...,(x^{(m)}, y^{(m)})\\} $, want $\\hat{y}^{(i)} \\approx y^{(i)}$\n",
    "- Loss (error) function: $L(\\hat{y}, y) = - ( ylog\\hat{y}+(1-y)log(1-\\hat{y}) )$\n",
    "    - if y = 1, $L(\\hat{y}, y) = - log\\hat{y}$\n",
    "    - if y = 0, $L(\\hat{y}, y) = - log(1-\\hat{y})$\n",
    "- Cost Function: $ J(w,b) = \\frac{1}{m} \\sum_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)})  = - \\frac{1}{m} \\sum_{i=1}^m [ \\, y^{(i)}log\\hat{y}^{(i)}+(1-y^{(i)})log(1-\\hat{y}^{(i)}) ] \\,$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Gradient Descent\n",
    "- $\\hat{y} = f(w^Tx + b)$ where $f(x) = \\frac{1}{1+e^{-x}}$\n",
    "- Cost Function: $ J(w,b) = \\frac{1}{m} \\sum_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)})  = - \\frac{1}{m} \\sum_{i=1}^m [ \\, y^{(i)}log\\hat{y}^{(i)}+(1-y^{(i)})log(1-\\hat{y}^{(i)}) ] \\,$\n",
    "- Repeat {w := w - $\\alpha \\frac{dJ(w,b)}{dw}$, and b := b - $\\alpha \\frac{dJ(w,b)}{db}$}\n",
    "    - $\\alpha$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Derivatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
